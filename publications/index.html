<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Maciek Tomczak</title> <meta name="author" content="Maciek Tomczak"> <meta name="description" content="Refereed conference and workshop publications"> <meta name="keywords" content="machine-learning, neural-audio-synthesis, neural-drum-synthesis, music-information-retrieval"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo_icon.png?a1ef7fd43982ead0c22275eecba2496e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://maciek-tomczak.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Maciek </span>Tomczak</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item"> <a class="nav-link" href="https://drive.google.com/open?id=10lnx5UwlpoNpXmMbkFBb5yGPgHof5FKq" target="_blank" rel="external nofollow noopener">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Refereed conference and workshop publications</p> </header> <article> <p>For a complete list, please check my <a href="https://scholar.google.com/citations?user=2upPg60AAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a>.</p> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ISMIR</abbr></div> <div id="tomczak2023virtuoso" class="col-sm-8"> <div class="title">A Dataset of String Ensemble Recordings and Onset Annotations for Timing Analysis</div> <div class="author"> <em>Maciek Tomczak</em>, Susan Min Li, and Massimiliano Di Luca</div> <div class="periodical"> <em>Extended Abstracts for the Late-Breaking Demo Session of the International Society of Music Information Retrieval Conference (ISMIR), Milan, Italy</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ismir2023program.ismir.net/lbd_312.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/arme-project/virtuoso-strings" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this paper, we present Virtuoso Strings, a dataset for timing analysis and automatic music transcription (AMT) tasks requiring note onset annotations. This dataset takes advantage of real-world recordings in multitrack format and is curated as a component of the Augmented Reality Music Ensemble (ARME) project, which investigates musician synchronisation and multimodal music analysis. The dataset comprises repeated recordings of quartet, trio, duet and solo ensemble performances. Each performance showcases varying temporal expressions and leadership role assignments, providing new possibilities for developing and evaluating AMT models across diverse musical styles. To reduce the cost of the labour-intensive manual annotation, a semi-automatic method was utilised for both annotation and quality control. The dataset features 746 tracks, totalling 68,728 onsets. Each track includes onset annotations for a single string instrument. This design facilitates the generation of audio files with varied instrument combinations for use in the AMT evaluation process.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2023virtuoso</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Li, Susan Min and Di Luca, Massimiliano}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Dataset of String Ensemble Recordings and Onset Annotations for Timing Analysis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Extended Abstracts for the Late-Breaking Demo Session of the International Society of Music Information Retrieval Conference (ISMIR), Milan, Italy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AudioMostly</abbr></div> <div id="tomczak2023onset" class="col-sm-8"> <div class="title">Onset Detection for String Instruments Using Bidirectional Temporal and Convolutional Recurrent Networks</div> <div class="author"> <em>Maciek Tomczak</em>, and Jason Hockman</div> <div class="periodical"> <em>Proceedings of the Audio Mostly Conference, Edinburgh, United Kingdom, ACM, New York, NY, USA</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3616195.3616206" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3616195.3616206" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent work in note onset detection has centered on deep learning models such as recurrent neural networks (RNN), convolutional neural networks (CNN) and more recently temporal convolutional networks (TCN), which achieve high evaluation accuracies for onsets characterized by clear, well-defined transients, as found in percussive instruments. However, onsets with less transient presence, as found in string instrument recordings, still pose a relatively difficult challenge for state-of-the-art algorithms. This challenge is further exacerbated by a paucity of string instrument data containing expert annotations. In this paper, we propose two new models for onset detection using bidirectional temporal and recurrent convolutional networks, which generalise to polyphonic signals and string instruments. We perform evaluations of the proposed methods alongside state-of-the-art algorithms for onset detection on a benchmark dataset from the MIR community, as well as on a test set from a newly proposed dataset of string instrument recordings with note onset annotations, comprising approximately 40 minutes and over 8,000 annotated onsets with varied expressive playing styles. The results demonstrate the effectiveness of both presented models, as they outperform the state-of-the-art algorithms on string recordings while maintaining comparative performance on other types of music.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2023onset</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Onset Detection for String Instruments Using Bidirectional Temporal and Convolutional Recurrent Networks}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the Audio Mostly Conference, Edinburgh, United Kingdom, ACM, New York, NY, USA}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{136--142}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">RPPW</abbr></div> <div id="enderby2023adaptive" class="col-sm-8"> <div class="title">Adaptive metronome: a MIDI plug-in for modelling cooperative timing in music ensembles</div> <div class="author"> Sean Enderby, Ryan Stables, Jason Hockman, <em>Maciek Tomczak</em>, Alan Wing, Mark Elliot, and Massimiliano Di Luca</div> <div class="periodical"> <em>Rhythm Production and Perception Workshop (RPPW), Birmingham, UK</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://e-space.mmu.ac.uk/633129/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/arme-project/AdaptiveMetronome" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">enderby2023adaptive</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Rhythm Production and Perception Workshop (RPPW), Birmingham, UK}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptive metronome: a MIDI plug-in for modelling cooperative timing in music ensembles}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Enderby, Sean and Stables, Ryan and Hockman, Jason and Tomczak, Maciek and Wing, Alan and Elliot, Mark and Di Luca, Massimiliano}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">RPPW</abbr></div> <div id="tomczak2023effect" class="col-sm-8"> <div class="title">Effect of leadership change on microtiming patterns in string quartet</div> <div class="author"> <em>Maciek Tomczak</em>, Min Susan Li, Maria Witek, and Jason Hockman</div> <div class="periodical"> <em>Rhythm Production and Perception Workshop (RPPW), Birmingham, UK</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://e-space.mmu.ac.uk/633128/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arme-project.co.uk/demos/microtiming" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2023effect</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Rhythm Production and Perception Workshop (RPPW), Birmingham, UK}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Effect of leadership change on microtiming patterns in string quartet}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Li, Min Susan and Witek, Maria and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PhD Thesis</abbr></div> <div id="tomczak2023automated" class="col-sm-8"> <div class="title">Automated Rhythmic Transformation of Drum Recordings</div> <div class="author"> <em>Maciek Tomczak</em> </div> <div class="periodical"> <em>Sound and Music Analysis (SOMA) Group, Digital Media Technology (DMT) Lab, Birmingham City University, UK</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">tomczak2023automated</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Sound and Music Analysis (SOMA) Group, Digital Media Technology (DMT) Lab, Birmingham City University, UK}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Rhythmic Transformation of Drum Recordings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JAES</abbr></div> <div id="cheshire2022deep" class="col-sm-8"> <div class="title">Deep Audio Effects for Snare Drum Recording Transformations</div> <div class="author"> Matthew Cheshire, Jake Drysdale, Sean Enderby, <em>Maciek Tomczak</em>, and Jason Hockman</div> <div class="periodical"> <em>Journal of the Audio Engineering Society (JAES), Special Issue: New Trends in Audio Effects</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cheshire2022deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Audio Effects for Snare Drum Recording Transformations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cheshire, Matthew and Drysdale, Jake and Enderby, Sean and Tomczak, Maciek and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of the Audio Engineering Society (JAES), Special Issue: New Trends in Audio Effects}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ISMIR</abbr></div> <div id="drysdale2021style" class="col-sm-8"> <div class="title">Style-based Drum Synthesis with GAN Inversion</div> <div class="author"> Jake Drysdale, <em>Maciek Tomczak</em>, and Jason Hockman</div> <div class="periodical"> <em>Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conference</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://archives.ismir.net/ismir2021/latebreaking/000041.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">drysdale2021style</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Style-based Drum Synthesis with GAN Inversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drysdale, Jake and Tomczak, Maciek and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACM MM</abbr></div> <div id="tomczak2020drum" class="col-sm-8"> <div class="title">Drum Synthesis and Rhythmic Transformation with Adversarial Autoencoders</div> <div class="author"> <em>Maciek Tomczak</em>, Masataka Goto, and Jason Hockman</div> <div class="periodical"> <em>Proceedings of the ACM International Conference on Multimedia (ACM-MM), Seattle, USA</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413519" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413519" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Creative rhythmic transformations of musical audio refer to automated methods for manipulation of temporally-relevant sounds in time. This paper presents a method for joint synthesis and rhythm transformation of drum sounds through the use of adversarial autoencoders (AAE). Users may navigate both the timbre and rhythm of drum patterns in audio recordings through expressive control over a low-dimensional latent space. The model is based on an AAE with Gaussian mixture latent distributions that introduce rhythmic pattern conditioning to represent a wide variety of drum performances. The AAE is trained on a dataset of bar-length segments of percussion recordings, along with their clustered rhythmic pattern labels. The decoder is conditioned during adversarial training for mixing of data-driven rhythmic and timbral properties. The system is trained with over 500000 bars from 5418 tracks in popular datasets covering various musical genres. In an evaluation using real percussion recordings, the reconstruction accuracy and latent space interpolation between drum performances are investigated for audio generation conditioned by target rhythmic patterns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2020drum</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Goto, Masataka and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Drum Synthesis and Rhythmic Transformation with Adversarial Autoencoders}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimedia (ACM-MM), Seattle, USA}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2427--2435}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">DAFx</abbr></div> <div id="drysdale2020adversarial" class="col-sm-8"> <div class="title">Adversarial Synthesis of Drum Sounds</div> <div class="author"> Jake Drysdale, <em>Maciek Tomczak</em>, and Jason Hockman</div> <div class="periodical"> <em>Proceedings of the International Conference on Digital Audio Effects (DAFx), Vienna, Austria</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">drysdale2020adversarial</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adversarial Synthesis of Drum Sounds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drysdale, Jake and Tomczak, Maciek and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Digital Audio Effects (DAFx), Vienna, Austria}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{167--172}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">DAFx</abbr></div> <div id="tomczak2019drum" class="col-sm-8"> <div class="title">Drum Translation for Timbral and Rhythmic Transformation</div> <div class="author"> <em>Maciek Tomczak</em>, Jake Drysdale, and Jason Hockman</div> <div class="periodical"> <em>Proceedings of the International Conference on Digital Audio Effects (DAFx), Birmingham, UK</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dafx.de/paper-archive/2019/DAFx2019_paper_25.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Many recent approaches to creative transformations of musical audio have been motivated by the success of raw audio generation models such as WaveNet, in which audio samples are modeled by generative neural networks. This paper describes a generative audio synthesis model for multi-drum translation based on a WaveNet denosing autoencoder architecture. The timbre of an arbitrary source audio input is transformed to sound as if it were played by various percussive instruments while preserving its rhythmic structure. Two evaluations of the transformations are conducted based on the capacity of the model to preserve the rhythmic patterns of the input and the audio quality as it relates to timbre of the target drum domain. The first evaluation measures the rhythmic similarities between the source audio and the corresponding drum translations, and the second provides a numerical analysis of the quality of the synthesised audio. Additionally, a semi- and fully-automatic audio effect has been proposed, in which the user may assist the system by manually labelling source audio segments or use a state-of-the-art automatic drum transcription system prior to drum translation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2019drum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Drum Translation for Timbral and Rhythmic Transformation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Drysdale, Jake and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Digital Audio Effects (DAFx), Birmingham, UK}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{340--346}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">DAFx</abbr></div> <div id="tomczak2018audio" class="col-sm-8"> <div class="title">Audio Style Transfer with Rhythmic Constraints</div> <div class="author"> <em>Maciek Tomczak</em>, Carl Southall, and Jason Hockman</div> <div class="periodical"> <em>Proceedings of the International Conference on Digital Audio Effects (DAFx), Aveiro, Portugal</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.dafx.de/paper-archive/2018/papers/DAFx2018_paper_48.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/maciek-tomczak/audio-style-transfer-with-rhythmic-constraints" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2018audio</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Southall, Carl and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Digital Audio Effects (DAFx), Aveiro, Portugal}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Audio Style Transfer with Rhythmic Constraints}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{45--50}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FMA</abbr></div> <div id="ali2018player" class="col-sm-8"> <div class="title">Player Recognition for Traditional Irish Flute Recordings</div> <div class="author"> Islah Ali-MacLachlan, Carl Southall, <em>Maciek Tomczak</em>, and Jason Hockman</div> <div class="periodical"> <em>In Proceedings of the 8th International Workshop on Folk Music Analysis, Thessaloniki, Greece</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://fma2018.mus.auth.gr/files/papers/FMA2018_paper_1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ali2018player</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Player Recognition for Traditional Irish Flute Recordings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ali-MacLachlan, Islah and Southall, Carl and Tomczak, Maciek and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 8th International Workshop on Folk Music Analysis, Thessaloniki, Greece}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3--8}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">RPPW</abbr></div> <div id="tomczak2017rhythm" class="col-sm-8"> <div class="title">Rhythm Modelling using Convolutional Neural Networks</div> <div class="author"> <em>Maciek Tomczak</em>, Carl Southall, and Jason Hockman</div> <div class="periodical"> <em>Rhythm Production and Perception Workshop (RPPW), Birmingham, UK</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2017rhythm</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Southall, Carl and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Rhythm Production and Perception Workshop (RPPW), Birmingham, UK}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rhythm Modelling using Convolutional Neural Networks}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FMA</abbr></div> <div id="ali2017improved" class="col-sm-8"> <div class="title">Improved Onset Detection for Traditional Flute Recordings using Convolutional Neural Networks</div> <div class="author"> Islah Ali-MacLachlan, <em>Maciek Tomczak</em>, Carl Southall, and Jason Hockman</div> <div class="periodical"> <em></em> 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://core.ac.uk/reader/141207270" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ali2017improved</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improved Onset Detection for Traditional Flute Recordings using Convolutional Neural Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ali-MacLachlan, Islah and Tomczak, Maciek and Southall, Carl and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 7th International Workshop on Folk Music Analysis, Malaga, Spain}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FMA</abbr></div> <div id="ali2016note" class="col-sm-8"> <div class="title">Note, Cut and Strike Detection for Traditional Irish Flute Recordings</div> <div class="author"> Islah Ali-MacLachlan, <em>Maciek Tomczak</em>, Carl Southall, and Jason Hockman</div> <div class="periodical"> <em></em> 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://core.ac.uk/download/pdf/141206238.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ali2016note</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Note, Cut and Strike Detection for Traditional Irish Flute Recordings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ali-MacLachlan, Islah and Tomczak, Maciek and Southall, Carl and Hockman, Jason}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 6th International Workshop on Folk Music Analysis, Dublin, Ireland}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{30--35}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="tomczak2015salience" class="col-sm-8"> <div class="title">The Salience of MFCC Semantic Classification on Electric Guitar Recordings</div> <div class="author"> <em>Maciek Tomczak</em>, and Ryan Stables</div> <div class="periodical"> <em>Undergraduate thesis. Birmingham City University</em>, 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This project describes an approach of semantic recognition by using the Mel Frequency Cepstral Coefficients (MFCCs) extracted from equalised signal of electric guitar recordings. Feature scaling is employed, prior to training and testing semantically processed samples via k Nearest Neighbour (kNN) and Support Vector Machine (SVM). Based on the created dataset of total 400 semantic trials collected from 20 experiment participants, it was possible to successfully train the kNN and SVM classifiers to distinguish between warm and bright extracted features. Results presented in this study show that a k = 5 NN model classifies the warm and bright descriptors most accurately, achieving 0.04% error on the test set.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tomczak2015salience</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Salience of MFCC Semantic Classification on Electric Guitar Recordings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tomczak, Maciek and Stables, Ryan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Undergraduate thesis. Birmingham City University}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Maciek Tomczak. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-TJ1FNYQNJH"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-TJ1FNYQNJH");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>